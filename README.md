# Human Aligned Autonomous Driving (HAAD)


As part of my studies at Duke University, I‚Äôve been exploring my interest in AI Alignment, specifically aligning human thinking with machine learning models. This research focuses on developing a human-aligned model to detect hazardous driving situations, ensuring that autonomous systems align more closely with how humans perceive risk on the road.

While AI alignment is important, it is not always the best solution. Although there are cases where aligning models with human judgment is necessary, the greater priority is ensuring models are interpretable and/or explainable. Regardless of alignment, all AI systems must remain transparent and ethically sound, particularly in safety-critical domains like autonomous driving.

In this research, we are exploring a key ethical question: Can we develop a human-aligned model that is both explainable and just as accurate as existing models that aren‚Äôt human-aligned? The goal is to create a model that integrates human judgment without compromising on performance, ensuring the system is both transparent and reliable.

üìù I need your help! Training these AI models requires substantial data, and your participation is crucial. Please take 5 minutes to complete the survey linked below and follow the instructions carefully. Your input will greatly assist in data collection and the advancement of this research.
